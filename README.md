<img src="docs\images\infnet-logo.png" width="200">


# Projeto da Disciplina de Engenharia de Machine Learning

Link do projeto:
[https://github.com/jorgeldn/infnet](https://github.com/jorgeldn/infnet)

## Overview

Desenvolver um preditor de arremessos usando duas abordagens (regress√£o e classifica√ß√£o) para prever se o "Black Mamba" (apelido de Kobe) acertou ou errou a cesta.
Na pasta `data/01_raw` est√£o disposiveis os arquivos: **dataset_kobe_dev.parquet** e **dataset_kobe_prod.parquet** alvos de estudo deste projeto.

## Configura√ß√£o do ambiente de desenvolvimento

Para executar o projeto, foi criado um ambiente virtual utilizando o **VENV** com o seguinte comando:

```
python -m venv venv-pos-ia
```

Foi gerado um arquivo `requirements.in` com as depend√™ncias do projeto.
O arquivo foi compilado com o comando `pip-compile requirements.in` e gerou um arquivo `requirements.txt`.

Para instala√ß√£o, executar:

```
pip install -r requirements.txt
```

## Quest√£o 1: Estrutura e solu√ß√£o do projeto?

Este projeto foi desenvolvido utilizando o framework Kedro versaÃÉo `kedro 0.19.11`.

<img src="docs\images\folders-structure.png" width="200">

## Quest√£o 2: Diagrama de etapas do projeto

<img src="docs\images\diagram.png">

## Quest√£o 3: Como as ferramentas Streamlit, MLFlow, PyCaret e Scikit-Learn auxiliam na constru√ß√£o dos pipelines?

### 1. **Rastreamento de Experimentos**
   - **MLflow**: Permite registrar e acompanhar diferentes experimentos de modelagem, armazenando m√©tricas, hiperpar√¢metros e artefatos de cada execu√ß√£o. Isso facilita a compara√ß√£o entre abordagens de regress√£o e classifica√ß√£o.
   - **PyCaret**: Automatiza a experimenta√ß√£o com diferentes modelos, armazenando m√©tricas e permitindo r√°pida compara√ß√£o entre t√©cnicas.

### 2. **Fun√ß√µes de Treinamento**
   - **Scikit-Learn**: Fornece bibliotecas para treinamento, valida√ß√£o e ajuste de modelos preditivos, incluindo algoritmos de regress√£o e classifica√ß√£o.
   - **PyCaret**: Facilita a sele√ß√£o autom√°tica dos melhores modelos, aplicando t√©cnicas como AutoML e tunagem de hiperpar√¢metros sem necessidade de configura√ß√£o manual.

### 3. **Monitoramento da Sa√∫de do Modelo**
   - **MLflow**: Possui funcionalidades de model registry e tracking, permitindo monitorar a degrada√ß√£o do modelo ao longo do tempo.
   - **Streamlit**: Pode ser utilizado para criar pain√©is interativos e visualizar m√©tricas do modelo em tempo real.

### 4. **Atualiza√ß√£o de Modelo**
   - **MLflow**: Possui um registry para gerenciar diferentes vers√µes dos modelos, facilitando a atualiza√ß√£o e rollback de vers√µes anteriores.
   - **PyCaret**: Implementa pipelines automatizados que incluem reentrenamento de modelos com novos dados.

### 5. **Provisionamento (Deployment)**
   - **MLflow**: Oferece integra√ß√£o com APIs REST para servir modelos como microservi√ßos.
   - **Streamlit**: Permite construir interfaces interativas para testar previs√µes do modelo de forma simples e r√°pida.

---

### **Impacto da Escolha de Treino e Teste no Modelo Final**
A forma como os dados de treino e teste s√£o divididos pode influenciar diretamente o desempenho e a capacidade de generaliza√ß√£o do modelo. Algumas considera√ß√µes importantes incluem:

1. **Representatividade dos Dados**  
   - Se os dados de treino n√£o forem representativos da distribui√ß√£o real dos dados, o modelo pode n√£o aprender padr√µes generaliz√°veis.
   - Se o conjunto de teste for muito diferente do de treino, o modelo pode ter um desempenho artificialmente ruim.

2. **Overfitting e Underfitting**  
   - **Overfitting:** O modelo pode memorizar padr√µes espec√≠ficos do conjunto de treino e ter um desempenho ruim em novos dados (teste).
   - **Underfitting:** Se o conjunto de treino for muito pequeno ou n√£o representativo, o modelo pode n√£o aprender padr√µes relevantes.

3. **Estratifica√ß√£o e Balanceamento**  
   - Se o conjunto de treino e teste n√£o mantiver a mesma propor√ß√£o das classes (no caso de classifica√ß√£o), o modelo pode aprender vi√©ses indesejados.

---

## Quest√£o 4

Com base no diagrama gerado, que ilustra um projeto usando **Kedro**, podemos identificar diversos artefatos criados ao longo do pipeline de dados. 
Abaixo est√° a lista dos principais **artefatos**, com uma descri√ß√£o detalhada da composi√ß√£o de cada um:

---

### 1. **dataset_kobe** (Cat√°logo: `raw`)
- **Tipo**: Fonte de dados bruta.
- **Composi√ß√£o**:
  - Arquivo Parquet.
  - Inclui todas as vari√°veis originais, ru√≠dos e poss√≠veis inconsist√™ncias.
  - Cadastrado no `catalog.yml` com tipo `raw`.

#### Estrutura da Tabela

| #  | Coluna               | Tipo     | Descri√ß√£o |
|----|----------------------|----------|-----------|
| 0  | action_type         | object   | Tipo espec√≠fico da a√ß√£o de arremesso (ex: Jump Shot, Layup, Dunk). |
| 1  | combined_shot_type  | object   | Tipo geral de arremesso (ex: Jump Shot, Bank Shot, Dunk). |
| 2  | game_event_id       | int64    | Identifica√ß√£o √∫nica do evento dentro da partida. |
| 3  | game_id             | int64    | Identifica√ß√£o √∫nica da partida. |
| 4  | lat                 | float64  | Latitude da posi√ß√£o do arremesso na quadra. |
| 5  | loc_x               | int64    | Coordenada X do local do arremesso em rela√ß√£o √† quadra. |
| 6  | loc_y               | int64    | Coordenada Y do local do arremesso em rela√ß√£o √† quadra. |
| 7  | lon                 | float64  | Longitude da posi√ß√£o do arremesso na quadra. |
| 8  | minutes_remaining   | int64    | Minutos restantes no quarto em que o arremesso foi realizado. |
| 9  | period              | int64    | N√∫mero do per√≠odo da partida (ex: 1, 2, 3, 4, OT). |
| 10 | playoffs           | int64    | Indica se a partida foi nos playoffs (1) ou na temporada regular (0). |
| 11 | season             | object   | Temporada do jogo no formato "YYYY-YY" (ex: "2001-02"). |
| 12 | seconds_remaining  | int64    | Segundos restantes no quarto em que o arremesso foi realizado. |
| 13 | shot_distance      | int64    | Dist√¢ncia do arremesso em p√©s. |
| 14 | shot_made_flag     | float64  | Indica se o arremesso foi convertido (1) ou n√£o (0). |
| 15 | shot_type          | object   | Tipo de arremesso baseado no n√∫mero de pontos (2PT Field Goal ou 3PT Field Goal). |
| 16 | shot_zone_area     | object   | √Årea da quadra onde o arremesso ocorreu (ex: Right Side, Left Side, Center). |
| 17 | shot_zone_basic    | object   | Classifica√ß√£o do tipo de zona do arremesso (ex: Restricted Area, Mid-Range, Backcourt). |
| 18 | shot_zone_range    | object   | Dist√¢ncia do arremesso (ex: 8-16 ft., 16-24 ft., 24+ ft.). |
| 19 | team_id            | int64    | Identifica√ß√£o √∫nica do time de Kobe Bryant (Los Angeles Lakers). |
| 20 | team_name          | object   | Nome do time (Los Angeles Lakers). |
| 21 | game_date          | object   | Data da partida no formato YYYYMMDD. |
| 22 | matchup           | object   | Identifica√ß√£o do confronto (ex: LAL @ BOS, LAL vs MIA). |
| 23 | opponent          | object   | Nome do time advers√°rio. |
| 24 | shot_id           | int64    | Identifica√ß√£o √∫nica do arremesso. |

#### Observa√ß√µes
- O dataset cont√©m 24.271 registros.
- A coluna `shot_made_flag` possui valores nulos, indicando arremessos cuja convers√£o n√£o foi informada.

---

### 2. **data_filtered** (Cat√°logo: `entrada`)
- **Tipo**: Dados pr√©-processados.
- **Composi√ß√£o**:
  - Resultado da limpeza e filtragem realizada pelo pipeline `PreparacaoDados` no node `prepare_data`.
  - As transforma√ß√µes incluem:
    - Remo√ß√£o de outliers ou nulos.
    - Convers√£o de tipos de dados.
  - Cadastrado no `catalog.yml` como um dataset intermedi√°rio.

#### Somente as colunas foram selecionadas para o `data_filtered`

| Coluna               | Tipo     | Descri√ß√£o |
|----------------------|----------|-----------|
| lat                 | float64  | Latitude da posi√ß√£o do arremesso na quadra. |
| lon                 | float64  | Longitude da posi√ß√£o do arremesso na quadra. |
| minutes_remaining   | int64    | Minutos restantes no quarto em que o arremesso foi realizado. |
| period              | int64    | N√∫mero do per√≠odo da partida (ex: 1, 2, 3, 4, OT). |
| playoffs           | int64    | Indica se a partida foi nos playoffs (1) ou na temporada regular (0). |
| shot_distance      | int64    | Dist√¢ncia do arremesso em p√©s. |

---

### 3. **base_train / base_test** (Cat√°logo: `entrada`)
- **Tipo**: Dados de treino e teste.
- **Composi√ß√£o**:
  - Conjunto de dados particionado a partir de `data_filtered`.
  - Usado para alimentar o node de `Treinamento`.
  - Armazenado separadamente em arquivos como `base_train.parquet` e `base_test.parquet`.

---

### 4. **Modelo Treinado** (registrado no MLflow)
- **Tipo**: Modelo de machine learning.
- **Composi√ß√£o**:
  - Objeto serializado dos modelos `Logistic Regression` e `Decision Tree Classifier`.
  - Metainforma√ß√µes como:
    - Hiperpar√¢metros usados.
    - M√©tricas de performance (ex: acur√°cia, F1-score).
    - Artefatos complementares como scaler, encoder, etc.
  - Registrado no **MLflow Model Registry** com versionamento.

---

### 5. **Registro no MLflow**
- **Tipo**: Metadata tracking.
- **Composi√ß√£o**:
  - Logs de execu√ß√£o do experimento.
  - Par√¢metros de input (hiperpar√¢metros).
  - M√©tricas de avalia√ß√£o.
  - Arquivos de sa√≠da como o modelo `.pkl`, gr√°ficos, etc.
  - Interface visual e API de consulta.

---

### 6. **Aplica√ß√£o Streamlit**
- **Tipo**: Interface web.
- **Composi√ß√£o**:
  - C√≥digo Python com l√≥gica de front-end interativo.
  - Elementos como:
    - Campos de input do usu√°rio.
    - L√≥gica de requisi√ß√£o ao modelo via MLflow (ou diretamente).
    - Exibi√ß√£o de outputs com probabilidades.
  - Conectada ao modelo treinado para infer√™ncia em tempo real.

---

### 7. **Nodes (Fun√ß√µes do pipeline)**
Embora n√£o sejam arquivos em si, os *nodes* s√£o artefatos de c√≥digo fundamentais:
- **PreparacaoDados**:
  - Fun√ß√£o respons√°vel por ingest√£o e limpeza dos dados brutos.
- **Treinamento**:
  - Fun√ß√µes que recebem os dados tratados e executa o treinamento dos modelos.

---
## Quest√£o 5
No prompt de comando, executado o seguinte comando:
```
kedro run --pipeline=PreparacaoDados
```

#### Sobre o dataset`data_filtered`
Dimens√£o do dataset ap√≥s limpeza:  20285 linhas e 7 colunas.

### **Implementa√ß√£o e Execu√ß√£o do Pipeline "PreparacaoDados"**
Todo o pipeline foi integrado com MLFlow para registro e acompanhamento dos experimentos.
<img src="docs\images\pipeline-run-preparacao-log.png">

Ao executar o pipeline, as seguintes m√©tricas foram geradas no MLflow:
<img src="docs\images\mlflow-preparacao.png">

E tamb√©m os arquivos gerados:
<img src="docs\images\pipeline-preparacao-artefatos.png">

### **Estrat√©gias para Minimizar o Vi√©s de Dados**
1. **Divis√£o Estratificada**  
   - Para problemas de classifica√ß√£o, a t√©cnica **stratified split** garante que a distribui√ß√£o da vari√°vel alvo seja semelhante nos conjuntos de treino e teste.  
   - Isso evita que o modelo aprenda padr√µes enviesados por distribui√ß√µes desbalanceadas.

   **Exemplo no Scikit-Learn:**  
   ```python
   from sklearn.model_selection import train_test_split

   train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["shot_made_flag"], random_state=42)
   ```

2. **Aumento de Dados (Data Augmentation)**  
   - Quando h√° poucas amostras de certas classes, pode-se gerar novos dados sint√©ticos para equilibrar as classes.
   - T√©cnicas como **SMOTE (Synthetic Minority Over-sampling Technique)** podem ser usadas para balancear classes desbalanceadas.

3. **Cross-Validation (Valida√ß√£o Cruzada)**  
   - Em vez de usar uma √∫nica divis√£o treino/teste, a valida√ß√£o cruzada (ex.: k-fold cross-validation) permite testar o modelo em diferentes divis√µes dos dados, reduzindo o risco de vi√©s na amostragem.

   **Exemplo de K-Fold Cross-Validation no Scikit-Learn:**  
   ```python
   from sklearn.model_selection import KFold, cross_val_score

   kf = KFold(n_splits=5, shuffle=True, random_state=42)
   scores = cross_val_score(model, X, y, cv=kf, scoring="accuracy")
   print(f"M√©dia das acur√°cias: {scores.mean()}")
   ```

4. **Remo√ß√£o de Dados Redundantes e Limpeza**  
   - Identificar e remover duplicatas ou dados inconsistentes pode evitar que o modelo aprenda padr√µes irrelevantes.

5. **Testar com Dados de Produ√ß√£o**  
   - Se poss√≠vel, testar o modelo com dados reais que ele encontrar√° na produ√ß√£o para garantir que os resultados sejam confi√°veis.

## Quest√£o 6

No prompt de comando, executado o seguinte comando:
```
kedro run --pipeline=Treinamento
```

O pipeline foi deviamente registrado no MLflow:
<img src="docs\images\mlflow-treinamento.png">

Durante a execu√ß√£o do pipeline, o seguinte output no console foi gerado:

---

#### **√Årvore de Decis√£o:**
<img src="docs\images\training-metrics-dt.png">

---

#### **Regeress√£o Log√≠stica:**
<img src="docs\images\training-metrics-lr.png">

---
Com base nas m√©tricas de valida√ß√£o cruzada (10 folds) apresentadas nas imagens dos dois modelos ‚Äî **Regress√£o Log√≠stica** e **√Årvore de Decis√£o** ‚Äî o modelo mais adequado para finaliza√ß√£o √©:

### ‚úÖ **Modelo Escolhido: Regress√£o Log√≠stica**

### üìä **Comparativo das principais m√©tricas (m√©dia dos folds)**

| M√©trica     | Regress√£o Log√≠stica | √Årvore de Decis√£o |
|-------------|----------------------|--------------------|
| **Accuracy** | **0.5781**           | 0.5264             |
| **AUC**      | **0.6085**           | 0.5160             |
| **Recall**   | 0.4921               | **0.5530**         |
| **Precision**| **0.5669**           | 0.5034             |
| **F1 Score** | **0.5267**           | 0.5142             |
| **Kappa**    | **0.1496**           | 0.0657             |
| **MCC**      | **0.1509**           | 0.0662             |

---

### üß† **Justificativa da escolha**

1. **Acur√°cia Geral Superior**: A regress√£o log√≠stica alcan√ßou uma m√©dia de acur√°cia 5 pontos percentuais acima da √°rvore de decis√£o (57.8% vs. 52.6%).

2. **Melhor Separa√ß√£o entre Classes (AUC)**: O AUC da regress√£o log√≠stica (0.6085) indica maior capacidade de distinguir as classes corretamente. O modelo de √°rvore tem AUC pr√≥ximo de 0.5, o que sugere desempenho similar ao aleat√≥rio.

3. **M√©tricas de Balanceamento (Kappa e MCC)**: Ambas s√£o substancialmente mais altas na regress√£o log√≠stica, o que refor√ßa que o modelo est√° aprendendo padr√µes √∫teis, e n√£o apenas se ajustando ao desbalanceamento ou aleatoriedade.

4. **Recall ligeiramente inferior, mas compensado**: Embora a √°rvore de decis√£o tenha maior *recall* (sensibilidade), ela perde em todas as outras m√©tricas, o que torna o modelo menos robusto como um todo.

---

## Quest√£o 7

Ap√≥s desenvolver e executar o pipeline, o seguinte erro foi gerado no console:
<img src="docs\images\pipeline-aplicacao-erro.png">

Significa que os tipos de dados do DataFrame de produ√ß√£o n√£o √© aderente ao de desenvolvimento. Isso ocorreu pois os tipos de dados passados para o modelo via predict n√£o bateram com a assinatura registrada do modelo.

Para contornar o problema de falha na execu√ß√£o, foi necess√°rio editar o c√≥digo para corrigir os tipos de dados.
<img src="docs\images\mlflow-aplicacao-plot.png">

### üìå **O que mudou entre a base de treino (`kobe_dev`) e a base de produ√ß√£o (`kobe_prod`)?**

Pelos ind√≠cios observados:

| Aspecto | Base de Treinamento (`dev`) | Base de Produ√ß√£o (`prod`) |
|--------|------------------------------|----------------------------|
| **Colunas** | 7 (features + target) | 24 (colunas completas do dataset original) |
| **Filtragem** | Pr√©-processada, com somente vari√°veis relevantes e sem nulos | Crua, sem filtragem, com valores faltantes no `shot_made_flag` |
| **Tipos de dados** | Todos como `float64`, compat√≠veis com MLflow | Muitos como `int64`, `object`, e com `NaN`s |

#### ‚ú≥Ô∏è Conclus√£o:
A base de produ√ß√£o **n√£o foi tratada com o mesmo pr√©-processamento** que a base de treino, o que pode causar inconsist√™ncia nas previs√µes e quebra de performance.

### ‚úÖ Justificativa

O modelo **n√£o √© diretamente aderente** √† base de produ√ß√£o por tr√™s motivos principais:

1. A produ√ß√£o n√£o passou por pr√©-processamento (tem campos extras, tipos errados e valores nulos).
2. As vari√°veis relevantes est√£o misturadas com outras n√£o usadas pelo modelo.
3. A presen√ßa de `NaN` no target impedia a avalia√ß√£o direta do desempenho.

Excelente pergunta ‚Äî isso toca no cora√ß√£o do **monitoramento de modelos em produ√ß√£o**, que √© essencial para garantir performance, confian√ßa e estabilidade ao longo do tempo.

Vamos abordar os dois cen√°rios que voc√™ mencionou:

---

### üß† **Monitoramento da sa√∫de do modelo**

A sa√∫de de um modelo pode ser monitorada em duas frentes principais:

- **Performance**: como o modelo est√° se saindo?
- **Dados**: os dados que entram no modelo continuam parecidos com os que ele foi treinado?


### ‚úÖ **1. Quando a vari√°vel resposta (target) est√° dispon√≠vel em produ√ß√£o**

Este √© o **melhor cen√°rio poss√≠vel**, pois permite **medir o desempenho real** do modelo com dados atualizados.

### üéØ O que pode ser monitorado:
- **M√©tricas de performance** como:
  - `log_loss`, `f1_score`, `accuracy`, `precision`, `recall`
- **Atrasos entre previs√£o e r√≥tulo real** (tempo de feedback)
- **Drift de performance**: comparar as m√©tricas com benchmarks anteriores

### üõ†Ô∏è Ferramentas e abordagens:
- Executar o **pipeline de aplica√ß√£o** com o target inclu√≠do
- Registrar as m√©tricas no MLflow ou ferramentas como Evidently, Prometheus, etc.
- Programar **dashboards** em Streamlit, Superset, Grafana ou Power BI

---

### ‚ö†Ô∏è **2. Quando a vari√°vel resposta N√ÉO est√° dispon√≠vel em produ√ß√£o**

Neste caso, voc√™ **n√£o pode medir diretamente a performance**. Mas ainda √© poss√≠vel monitorar **a integridade e a ader√™ncia dos dados**.

### üîé O que monitorar:

#### a) **Data Drift (mudan√ßa nos dados de entrada)**
- Mudan√ßa na distribui√ß√£o das vari√°veis
- Novos valores em vari√°veis categ√≥ricas
- Mudan√ßa de m√©dia, mediana, desvio padr√£o

#### b) **Feature Importance Drift**
- Ver se a import√¢ncia das vari√°veis mudou muito com o tempo

#### c) **Score Stability**
- A distribui√ß√£o das probabilidades do modelo ao longo do tempo
- Ex: m√©dia da probabilidade de classe 1

### üõ†Ô∏è Ferramentas e abordagens:
- Usar **Evidently** (Python) para:
  - Data Drift Report
  - Targetless Monitoring
- **Alerts autom√°ticos** se alguma feature sair do intervalo de confian√ßa
- Logar a entrada do modelo com timestamp para auditoria futura


### üìä Exemplo pr√°tico de m√©tricas monitor√°veis sem `y`:

| M√©trica                 | Com `y` | Sem `y` |
|------------------------|---------|---------|
| Accuracy / F1 Score    | ‚úÖ      | ‚ùå      |
| Log Loss               | ‚úÖ      | ‚ùå      |
| Probabilidade m√©dia    | ‚úÖ/‚ùå   | ‚úÖ      |
| Mudan√ßa de distribui√ß√£o| ‚úÖ/‚ùå   | ‚úÖ      |
| Tempo de predi√ß√£o      | ‚úÖ      | ‚úÖ      |
| Taxa de erros t√©cnicos | ‚úÖ      | ‚úÖ      |


### üß© Conclus√£o

| Situa√ß√£o                     | O que monitorar                                |
|-----------------------------|------------------------------------------------|
| **Com target dispon√≠vel**   | M√©tricas de performance + integridade de dados |
| **Sem target dispon√≠vel**   | Drift de dados + distribui√ß√£o de scores        |

Ambos os cen√°rios exigem a√ß√µes autom√°ticas como **logs, alertas e auditorias**, e ferramentas como **MLflow, Evidently, Prometheus e Streamlit** ajudam nesse processo.

---
Excelente quest√£o! Quando colocamos um modelo em produ√ß√£o, √© fundamental definir **estrat√©gias de retreinamento** para garantir que ele continue relevante, preciso e confi√°vel com o passar do tempo.

Essas estrat√©gias podem ser divididas em **duas abordagens principais**:

---

### üîÅ **1. Estrat√©gia Reativa de Retreinamento**

> O modelo s√≥ √© retreinado **quando h√° sinais claros de degrada√ß√£o**.

#### üìå Caracter√≠sticas:
- Baseada em **monitoramento constante** da performance
- Retreinamento **acontece ap√≥s** o modelo apresentar queda nas m√©tricas
- Depende da **disponibilidade da vari√°vel resposta (target)**

#### üß† Exemplos de gatilhos:
- F1 Score ou Accuracy caiu abaixo de um limite (ex: 5% abaixo do benchmark)
- Log Loss subiu acima de um limiar
- Aumento de reclama√ß√µes ou erros operacionais
- Atraso no tempo de resposta devido a complexidade crescente

#### ‚úÖ Vantagens:
- Evita retrabalho desnecess√°rio
- √â eficiente quando h√° **feedback cont√≠nuo** do target

#### ‚ùå Desvantagens:
- √â **reativa**: o modelo j√° degradou quando o retreinamento come√ßa
- Pode gerar **impactos negativos** antes da corre√ß√£o (ex: perda de vendas, decis√µes ruins)

---

### üìÖ **2. Estrat√©gia Preditiva (ou Proativa) de Retreinamento**

> O modelo √© retreinado **em ciclos planejados ou com base em sinais antecipados**, mesmo sem queda vis√≠vel de performance.

#### üìå Caracter√≠sticas:
- Foco em **prevenir** problemas futuros
- Usa an√°lise de **data drift** ou **mudan√ßa de contexto**
- Pode ser baseada em **agendamento (ex: mensal, trimestral)**

#### üß† Exemplos de gatilhos:
- Mudan√ßa estat√≠stica significativa nas vari√°veis de entrada (data drift)
- Aumento na frequ√™ncia de valores ausentes, anomalias ou categorias novas
- Modelo em produ√ß√£o atingiu um n√∫mero X de novas amostras
- **Janela de tempo definida** (retraining a cada 60 dias, por exemplo)

#### ‚úÖ Vantagens:
- Mant√©m o modelo sempre fresco e adaptado
- Reduz risco de deteriora√ß√£o s√∫bita

#### ‚ùå Desvantagens:
- Pode ser **custoso** (mais consumo computacional e tempo de valida√ß√£o)
- Risco de **overfitting ao novo contexto** se n√£o for bem gerenciado

---

#### üß© Comparativo das Estrat√©gias

| Aspecto                    | Estrat√©gia Reativa        | Estrat√©gia Preditiva         |
|---------------------------|---------------------------|------------------------------|
| Base                      | M√©tricas de performance   | Tempo, volume ou sinais de drift |
| Exige `target`?           | Sim                       | N√£o necessariamente          |
| A√ß√£o                      | Ap√≥s problema aparecer    | Antes do problema acontecer  |
| Custo computacional       | Mais baixo                | Mais alto                    |
| Risco de impacto negativo | Maior                     | Menor                        |

---

### Streamlit App

Foi implementada uma **streamlit app** para visualiza√ß√£o dos dados e monitoramento do modelo.
O App possui 2 abas:

- **Aba 1**: Previs√£o --> (Consome a API do MLflow)
- **Aba 2**: Monitoramento da Opera√ß√£o

> Para executar a app, basta rodar o arquivo `streamlit/app.py` com o comando `streamlit run app.py`.

<img src="docs\images\streamlit-aba-prev.png">
<img src="docs\images\streamlit-aba-mon-01.png">
<img src="docs\images\streamlit-aba-mon-02.png">
<img src="docs\images\streamlit-aba-mon-03.png">