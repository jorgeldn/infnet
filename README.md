<img src="docs\images\infnet-logo.png" width="200">


# Projeto da Disciplina de Engenharia de Machine Learning

Link do projeto:
[https://github.com/jorgeldn/infnet](https://github.com/jorgeldn/infnet)

## Overview

Desenvolver um preditor de arremessos usando duas abordagens (regress√£o e classifica√ß√£o) para prever se o "Black Mamba" (apelido de Kobe) acertou ou errou a cesta.
Na pasta `data/01_raw` est√£o disposiveis os arquivos: **dataset_kobe_dev.parquet** e **dataset_kobe_prod.parquet** alvos de estudo deste projeto.

## Depend√™ncias

Para executar o projeto, foi criado um ambiente virtual exclusivo a partir do comando:

```
python -m venv venv-pos-ia
```

Foi gerado um arquivo `requirements.in` com as depend√™ncias do projeto.
O arquivo foi compilado com o comando `pip-compile requirements.in` e gerou um arquivo `requirements.txt`.

Para instala√ß√£o, executar:

```
pip install -r requirements.txt
```

## Quest√£o 1: Estrutura e solu√ß√£o do projeto?

Este projeto foi desenvolvido utilizando o framework Kedro versaÃÉo `kedro 0.19.11`.

<img src="docs\images\folders-structure.png" width="200">

## Quest√£o 2: Diagrama de etapas do projeto

<img src="docs\images\diagram.png">

## Quest√£o 3: Como as ferramentas Streamlit, MLFlow, PyCaret e Scikit-Learn auxiliam na constru√ß√£o dos pipelines?

### 1. **Rastreamento de Experimentos**
   - **MLflow**: Permite registrar e acompanhar diferentes experimentos de modelagem, armazenando m√©tricas, hiperpar√¢metros e artefatos de cada execu√ß√£o. Isso facilita a compara√ß√£o entre abordagens de regress√£o e classifica√ß√£o.
   - **PyCaret**: Automatiza a experimenta√ß√£o com diferentes modelos, armazenando m√©tricas e permitindo r√°pida compara√ß√£o entre t√©cnicas.

### 2. **Fun√ß√µes de Treinamento**
   - **Scikit-Learn**: Fornece bibliotecas para treinamento, valida√ß√£o e ajuste de modelos preditivos, incluindo algoritmos de regress√£o e classifica√ß√£o.
   - **PyCaret**: Facilita a sele√ß√£o autom√°tica dos melhores modelos, aplicando t√©cnicas como AutoML e tunagem de hiperpar√¢metros sem necessidade de configura√ß√£o manual.

### 3. **Monitoramento da Sa√∫de do Modelo**
   - **MLflow**: Possui funcionalidades de model registry e tracking, permitindo monitorar a degrada√ß√£o do modelo ao longo do tempo.
   - **Streamlit**: Pode ser utilizado para criar pain√©is interativos e visualizar m√©tricas do modelo em tempo real.

### 4. **Atualiza√ß√£o de Modelo**
   - **MLflow**: Possui um registry para gerenciar diferentes vers√µes dos modelos, facilitando a atualiza√ß√£o e rollback de vers√µes anteriores.
   - **PyCaret**: Implementa pipelines automatizados que incluem reentrenamento de modelos com novos dados.

### 5. **Provisionamento (Deployment)**
   - **MLflow**: Oferece integra√ß√£o com APIs REST para servir modelos como microservi√ßos.
   - **Streamlit**: Permite construir interfaces interativas para testar previs√µes do modelo de forma simples e r√°pida.

---

### **Impacto da Escolha de Treino e Teste no Modelo Final**
A forma como os dados de treino e teste s√£o divididos pode influenciar diretamente o desempenho e a capacidade de generaliza√ß√£o do modelo. Algumas considera√ß√µes importantes incluem:

1. **Representatividade dos Dados**  
   - Se os dados de treino n√£o forem representativos da distribui√ß√£o real dos dados, o modelo pode n√£o aprender padr√µes generaliz√°veis.
   - Se o conjunto de teste for muito diferente do de treino, o modelo pode ter um desempenho artificialmente ruim.

2. **Overfitting e Underfitting**  
   - **Overfitting:** O modelo pode memorizar padr√µes espec√≠ficos do conjunto de treino e ter um desempenho ruim em novos dados (teste).
   - **Underfitting:** Se o conjunto de treino for muito pequeno ou n√£o representativo, o modelo pode n√£o aprender padr√µes relevantes.

3. **Estratifica√ß√£o e Balanceamento**  
   - Se o conjunto de treino e teste n√£o mantiver a mesma propor√ß√£o das classes (no caso de classifica√ß√£o), o modelo pode aprender vi√©ses indesejados.

---

## Quest√£o 4

Com base no diagrama gerado, que ilustra um projeto usando **Kedro**, podemos identificar diversos artefatos criados ao longo do pipeline de dados. 
Abaixo est√° a lista dos principais **artefatos**, com uma descri√ß√£o detalhada da composi√ß√£o de cada um:

---

### 1. **dataset_kobe** (Cat√°logo: `raw`)
- **Tipo**: Fonte de dados bruta.
- **Composi√ß√£o**:
  - Arquivo CSV, Excel, JSON, banco de dados ou API contendo dados n√£o tratados.
  - Inclui todas as vari√°veis originais, ru√≠dos e poss√≠veis inconsist√™ncias.
  - Cadastrado no `catalog.yml` com tipo `raw`.

---

### 2. **data_filtered**
- **Tipo**: Dados pr√©-processados.
- **Composi√ß√£o**:
  - Resultado da limpeza e filtragem realizada pelo node `PreparacaoDados`.
  - Pode incluir:
    - Remo√ß√£o de outliers ou nulos.
    - Convers√£o de tipos de dados.
    - Aplica√ß√£o de filtros l√≥gicos (ex: apenas jogadas feitas em partidas oficiais).
  - Cadastrado no `catalog.yml` como um dataset intermedi√°rio.

---

### 3. **base_train / base_test** (Cat√°logo: `entrada`)
- **Tipo**: Dados de treino e teste.
- **Composi√ß√£o**:
  - Conjunto de dados particionado a partir de `data_filtered`.
  - Pode incluir engenharia de atributos, normaliza√ß√£o, codifica√ß√£o categ√≥rica etc.
  - Usado para alimentar o node de `Treinamento`.
  - Armazenado separadamente em arquivos como `base_train.parquet` e `base_test.parquet`.

---

### 4. **Modelo Treinado** (registrado no MLflow)
- **Tipo**: Modelo de machine learning.
- **Composi√ß√£o**:
  - Objeto serializado do modelo (ex: `sklearn`, `xgboost`, etc.).
  - Metainforma√ß√µes como:
    - Hiperpar√¢metros usados.
    - M√©tricas de performance (ex: acur√°cia, F1-score).
    - Artefatos complementares como scaler, encoder, etc.
  - Registrado no **MLflow Model Registry** com versionamento.

---

### 5. **Registro no MLflow**
- **Tipo**: Metadata tracking.
- **Composi√ß√£o**:
  - Logs de execu√ß√£o do experimento.
  - Par√¢metros de input (hiperpar√¢metros).
  - M√©tricas de avalia√ß√£o.
  - Arquivos de sa√≠da como o modelo `.pkl`, gr√°ficos, etc.
  - Interface visual e API de consulta.

---

### 6. **Aplica√ß√£o Streamlit**
- **Tipo**: Interface web.
- **Composi√ß√£o**:
  - C√≥digo Python com l√≥gica de front-end interativo.
  - Elementos como:
    - Campos de input do usu√°rio.
    - L√≥gica de requisi√ß√£o ao modelo via MLflow (ou diretamente).
    - Exibi√ß√£o de outputs como previs√£o, gr√°ficos, explica√ß√µes.
  - Conectada ao modelo treinado para infer√™ncia em tempo real.

---

### 7. **Nodes (Fun√ß√µes do pipeline)**
Embora n√£o sejam arquivos em si, os *nodes* s√£o artefatos de c√≥digo fundamentais:
- **PreparacaoDados**:
  - Fun√ß√£o respons√°vel por ingest√£o e limpeza dos dados brutos.
- **Treinamento**:
  - Fun√ß√£o que recebe os dados tratados e executa o treinamento do modelo.

---
## Quest√£o 5

### **Implementa√ß√£o e Execu√ß√£o do Pipeline "PreparacaoDados"**
Todo o pipeline foi integrado com MLFlow para registro e acompanhamento dos experimentos.
<img src="docs\images\pipeline-run-preparacao-log.png">

Ao executar o pipeline, as seguintes m√©tricas foram geradas no MLflow:
<img src="docs\images\mlflow-preparacao.png">

E tamb√©m os arquivos gerados:
<img src="docs\images\pipeline-preparacao-artefatos.png">

### **Estrat√©gias para Minimizar o Vi√©s de Dados**
1. **Divis√£o Estratificada**  
   - Para problemas de classifica√ß√£o, a t√©cnica **stratified split** garante que a distribui√ß√£o da vari√°vel alvo seja semelhante nos conjuntos de treino e teste.  
   - Isso evita que o modelo aprenda padr√µes enviesados por distribui√ß√µes desbalanceadas.

   **Exemplo no Scikit-Learn:**  
   ```python
   from sklearn.model_selection import train_test_split

   train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["shot_made_flag"], random_state=42)
   ```

2. **Aumento de Dados (Data Augmentation)**  
   - Quando h√° poucas amostras de certas classes, pode-se gerar novos dados sint√©ticos para equilibrar as classes.
   - T√©cnicas como **SMOTE (Synthetic Minority Over-sampling Technique)** podem ser usadas para balancear classes desbalanceadas.

3. **Cross-Validation (Valida√ß√£o Cruzada)**  
   - Em vez de usar uma √∫nica divis√£o treino/teste, a valida√ß√£o cruzada (ex.: k-fold cross-validation) permite testar o modelo em diferentes divis√µes dos dados, reduzindo o risco de vi√©s na amostragem.

   **Exemplo de K-Fold Cross-Validation no Scikit-Learn:**  
   ```python
   from sklearn.model_selection import KFold, cross_val_score

   kf = KFold(n_splits=5, shuffle=True, random_state=42)
   scores = cross_val_score(model, X, y, cv=kf, scoring="accuracy")
   print(f"M√©dia das acur√°cias: {scores.mean()}")
   ```

4. **Remo√ß√£o de Dados Redundantes e Limpeza**  
   - Identificar e remover duplicatas ou dados inconsistentes pode evitar que o modelo aprenda padr√µes irrelevantes.

5. **Testar com Dados de Produ√ß√£o**  
   - Se poss√≠vel, testar o modelo com dados reais que ele encontrar√° na produ√ß√£o para garantir que os resultados sejam confi√°veis.

## Quest√£o 6

O pipeline foi deviamente registrado no MLflow:
<img src="docs\images\mlflow-treinamento.png">

Durante a execu√ß√£o do pipeline, o seguinte output no console foi gerado:

---

#### **√Årvore de Decis√£o:**
<img src="docs\images\training-metrics-dt.png">

---

#### **Regeress√£o Log√≠stica:**
<img src="docs\images\training-metrics-lr.png">

---
Com base nas m√©tricas de valida√ß√£o cruzada (10 folds) apresentadas nas imagens dos dois modelos ‚Äî **Regress√£o Log√≠stica** e **√Årvore de Decis√£o** ‚Äî o modelo mais adequado para finaliza√ß√£o √©:

### ‚úÖ **Modelo Escolhido: Regress√£o Log√≠stica**

### üìä **Comparativo das principais m√©tricas (m√©dia dos folds)**

| M√©trica     | Regress√£o Log√≠stica | √Årvore de Decis√£o |
|-------------|----------------------|--------------------|
| **Accuracy** | **0.5781**           | 0.5264             |
| **AUC**      | **0.6085**           | 0.5160             |
| **Recall**   | 0.4921               | **0.5530**         |
| **Precision**| **0.5669**           | 0.5034             |
| **F1 Score** | **0.5267**           | 0.5142             |
| **Kappa**    | **0.1496**           | 0.0657             |
| **MCC**      | **0.1509**           | 0.0662             |

---

### üß† **Justificativa da escolha**

1. **Acur√°cia Geral Superior**: A regress√£o log√≠stica alcan√ßou uma m√©dia de acur√°cia 5 pontos percentuais acima da √°rvore de decis√£o (57.8% vs. 52.6%).

2. **Melhor Separa√ß√£o entre Classes (AUC)**: O AUC da regress√£o log√≠stica (0.6085) indica maior capacidade de distinguir as classes corretamente. O modelo de √°rvore tem AUC pr√≥ximo de 0.5, o que sugere desempenho similar ao aleat√≥rio.

3. **M√©tricas de Balanceamento (Kappa e MCC)**: Ambas s√£o substancialmente mais altas na regress√£o log√≠stica, o que refor√ßa que o modelo est√° aprendendo padr√µes √∫teis, e n√£o apenas se ajustando ao desbalanceamento ou aleatoriedade.

4. **Recall ligeiramente inferior, mas compensado**: Embora a √°rvore de decis√£o tenha maior *recall* (sensibilidade), ela perde em todas as outras m√©tricas, o que torna o modelo menos robusto como um todo.
