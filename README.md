<img src="docs\images\infnet-logo.png" width="200">

# Projeto da Disciplina de Engenharia de Machine Learning

---
## Aluno: Jorge Luiz do Nascimento JÃºnior
Git do projeto:
[https://github.com/jorgeldn/infnet](https://github.com/jorgeldn/infnet)

## ğŸ“œ Overview

Desenvolver um preditor de arremessos usando duas abordagens (regressÃ£o e classificaÃ§Ã£o) para prever se o "Black Mamba" (apelido de Kobe) acertou ou errou a cesta.
Na pasta `data/01_raw` estÃ£o disposiveis os arquivos: **dataset_kobe_dev.parquet** e **dataset_kobe_prod.parquet** alvos de estudo deste projeto.

## ğŸŒ ConfiguraÃ§Ã£o do ambiente de desenvolvimento

Para desenvolver e executar o projeto de disciplina, foi instalado o `python 3.11` e criado um ambiente virtual utilizando o **VENV** com o seguinte comando:

```bash
python -m venv venv-pos-ia
```

Foi gerado um arquivo `requirements.in` com as dependÃªncias do projeto.
O arquivo foi compilado com o comando:

```bash
pip-compile requirements.in
``` 

gerando o arquivo `requirements.txt`.

Para instalaÃ§Ã£o de todas as bibliotecas, executar:

```bash
pip install -r requirements.txt
```

## ğŸ¯QuestÃ£o 1: Estrutura e soluÃ§Ã£o do projeto?

---
Este projeto foi desenvolvido utilizando o framework Kedro versaÌƒo `kedro 0.19.11`.

O projeto segue a seguinte estrutura:
```
ğŸ“‚ .  
â”œâ”€â”€ ğŸ“‚ conf  
â”‚   â”œâ”€â”€ ğŸ“‚ base  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ catalog.yml  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ parameters.yml 
â”œâ”€â”€ ğŸ“‚ data  
â”‚   â”œâ”€â”€ ğŸ“‚ 01_raw  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dataset_kobe_dev.parquet  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dataset_kobe_prod.parquet  
â”‚   â”œâ”€â”€ ğŸ“‚ 02_intermediate  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ base_test.parquet  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ base_train.parquet  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ data_filtered.parquet  
â”‚   â”œâ”€â”€ ğŸ“‚ 03_primary  
â”‚   â”œâ”€â”€ ğŸ“‚ 04_feature  
â”‚   â”œâ”€â”€ ğŸ“‚ 05_model_input  
â”‚   â”œâ”€â”€ ğŸ“‚ 06_models  
â”‚   â”œâ”€â”€ ğŸ“‚ 07_model_output  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ resultados_aplicacao.parquet  
â”‚   â”œâ”€â”€ ğŸ“‚ 08_reporting  
â”œâ”€â”€ ğŸ“‚ docs  
â”‚   â”œâ”€â”€ ğŸ“‚ images
â”œâ”€â”€ ğŸ“‚ mlruns  
â”œâ”€â”€ ğŸ“‚ notebooks  
â”œâ”€â”€ ğŸ“‚ src  
â”‚   â”œâ”€â”€ ğŸ“‚ infnet  
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ pipelines  
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ Aplicacao  
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py  
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ nodes.py  
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ pipeline.py  
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ PreparacaoDados  
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py  
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ nodes.py  
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ pipeline.py  
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ Treinamento  
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py  
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ nodes.py  
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ pipeline.py   
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sklearn_proba_wrapper.py  
â”œâ”€â”€ ğŸ“‚ streamlit  
â”‚   â”œâ”€â”€ ğŸ“„ app.py  
â”œâ”€â”€ ğŸ“‚ tests   
â”œâ”€â”€ ğŸ“„ pyproject.toml  
â”œâ”€â”€ ğŸ“„ README.md  
â”œâ”€â”€ ğŸ“„ requirements.in  
â”œâ”€â”€ ğŸ“„ requirements.txt
```
## ğŸ¯QuestÃ£o 2: Diagrama de etapas do projeto

---
<img src="docs\images\diagram.png">

## ğŸ¯QuestÃ£o 3: Como as ferramentas Streamlit, MLFlow, PyCaret e Scikit-Learn auxiliam na construÃ§Ã£o dos pipelines?

---
### 1. **Rastreamento de Experimentos**
   - **MLflow**: Permite registrar e acompanhar diferentes experimentos de modelagem, armazenando mÃ©tricas, hiperparÃ¢metros e artefatos de cada execuÃ§Ã£o. Isso facilita a comparaÃ§Ã£o entre abordagens de regressÃ£o e classificaÃ§Ã£o.
   - **PyCaret**: Automatiza a experimentaÃ§Ã£o com diferentes modelos, armazenando mÃ©tricas e permitindo rÃ¡pida comparaÃ§Ã£o entre tÃ©cnicas.

### 2. **FunÃ§Ãµes de Treinamento**
   - **Scikit-Learn**: Fornece bibliotecas para treinamento, validaÃ§Ã£o e ajuste de modelos preditivos, incluindo algoritmos de regressÃ£o e classificaÃ§Ã£o.
   - **PyCaret**: Facilita a seleÃ§Ã£o automÃ¡tica dos melhores modelos, aplicando tÃ©cnicas como AutoML e tunagem de hiperparÃ¢metros sem necessidade de configuraÃ§Ã£o manual.

### 3. **Monitoramento da SaÃºde do Modelo**
   - **MLflow**: Possui funcionalidades de model registry e tracking, permitindo monitorar a degradaÃ§Ã£o do modelo ao longo do tempo.
   - **Streamlit**: Pode ser utilizado para criar painÃ©is interativos e visualizar mÃ©tricas do modelo em tempo real.

### 4. **AtualizaÃ§Ã£o de Modelo**
   - **MLflow**: Possui um registry para gerenciar diferentes versÃµes dos modelos, facilitando a atualizaÃ§Ã£o e rollback de versÃµes anteriores.
   - **PyCaret**: Implementa pipelines automatizados que incluem reentrenamento de modelos com novos dados.

### 5. **Provisionamento (Deployment)**
   - **MLflow**: Oferece integraÃ§Ã£o com APIs REST para servir modelos como microserviÃ§os.
   - **Streamlit**: Permite construir interfaces interativas para testar previsÃµes do modelo de forma simples e rÃ¡pida.


### ğŸ“Œ**Impacto da Escolha de Treino e Teste no Modelo Final**
A forma como os dados de treino e teste sÃ£o divididos pode influenciar diretamente o desempenho e a capacidade de generalizaÃ§Ã£o do modelo. Algumas consideraÃ§Ãµes importantes incluem:

1. **Representatividade dos Dados**  
   - Se os dados de treino nÃ£o forem representativos da distribuiÃ§Ã£o real dos dados, o modelo pode nÃ£o aprender padrÃµes generalizÃ¡veis.
   - Se o conjunto de teste for muito diferente do de treino, o modelo pode ter um desempenho artificialmente ruim.

2. **Overfitting e Underfitting**  
   - **Overfitting:** O modelo pode memorizar padrÃµes especÃ­ficos do conjunto de treino e ter um desempenho ruim em novos dados (teste).
   - **Underfitting:** Se o conjunto de treino for muito pequeno ou nÃ£o representativo, o modelo pode nÃ£o aprender padrÃµes relevantes.

3. **EstratificaÃ§Ã£o e Balanceamento**  
   - Se o conjunto de treino e teste nÃ£o mantiver a mesma proporÃ§Ã£o das classes (no caso de classificaÃ§Ã£o), o modelo pode aprender viÃ©ses indesejados.

## ğŸ¯QuestÃ£o 4

---

Com base no diagrama gerado, que ilustra um projeto usando **Kedro**, podemos identificar diversos artefatos criados ao longo do pipeline de dados. 
Abaixo estÃ¡ a lista dos principais **artefatos**, com uma descriÃ§Ã£o detalhada da composiÃ§Ã£o de cada um:

### 1. ğŸ“‘ **dataset_kobe** (CatÃ¡logo: `raw`)
- **Tipo**: Fonte de dados bruta.
- **ComposiÃ§Ã£o**:
  - Arquivo Parquet.
  - Inclui todas as variÃ¡veis originais, ruÃ­dos e possÃ­veis inconsistÃªncias.
  - Cadastrado no `catalog.yml` com tipo `raw`.

#### Estrutura da Tabela

| #  | Coluna               | Tipo     | DescriÃ§Ã£o |
|----|----------------------|----------|-----------|
| 0  | action_type         | object   | Tipo especÃ­fico da aÃ§Ã£o de arremesso (ex: Jump Shot, Layup, Dunk). |
| 1  | combined_shot_type  | object   | Tipo geral de arremesso (ex: Jump Shot, Bank Shot, Dunk). |
| 2  | game_event_id       | int64    | IdentificaÃ§Ã£o Ãºnica do evento dentro da partida. |
| 3  | game_id             | int64    | IdentificaÃ§Ã£o Ãºnica da partida. |
| 4  | lat                 | float64  | Latitude da posiÃ§Ã£o do arremesso na quadra. |
| 5  | loc_x               | int64    | Coordenada X do local do arremesso em relaÃ§Ã£o Ã  quadra. |
| 6  | loc_y               | int64    | Coordenada Y do local do arremesso em relaÃ§Ã£o Ã  quadra. |
| 7  | lon                 | float64  | Longitude da posiÃ§Ã£o do arremesso na quadra. |
| 8  | minutes_remaining   | int64    | Minutos restantes no quarto em que o arremesso foi realizado. |
| 9  | period              | int64    | NÃºmero do perÃ­odo da partida (ex: 1, 2, 3, 4, OT). |
| 10 | playoffs           | int64    | Indica se a partida foi nos playoffs (1) ou na temporada regular (0). |
| 11 | season             | object   | Temporada do jogo no formato "YYYY-YY" (ex: "2001-02"). |
| 12 | seconds_remaining  | int64    | Segundos restantes no quarto em que o arremesso foi realizado. |
| 13 | shot_distance      | int64    | DistÃ¢ncia do arremesso em pÃ©s. |
| 14 | shot_made_flag     | float64  | Indica se o arremesso foi convertido (1) ou nÃ£o (0). |
| 15 | shot_type          | object   | Tipo de arremesso baseado no nÃºmero de pontos (2PT Field Goal ou 3PT Field Goal). |
| 16 | shot_zone_area     | object   | Ãrea da quadra onde o arremesso ocorreu (ex: Right Side, Left Side, Center). |
| 17 | shot_zone_basic    | object   | ClassificaÃ§Ã£o do tipo de zona do arremesso (ex: Restricted Area, Mid-Range, Backcourt). |
| 18 | shot_zone_range    | object   | DistÃ¢ncia do arremesso (ex: 8-16 ft., 16-24 ft., 24+ ft.). |
| 19 | team_id            | int64    | IdentificaÃ§Ã£o Ãºnica do time de Kobe Bryant (Los Angeles Lakers). |
| 20 | team_name          | object   | Nome do time (Los Angeles Lakers). |
| 21 | game_date          | object   | Data da partida no formato YYYYMMDD. |
| 22 | matchup           | object   | IdentificaÃ§Ã£o do confronto (ex: LAL @ BOS, LAL vs MIA). |
| 23 | opponent          | object   | Nome do time adversÃ¡rio. |
| 24 | shot_id           | int64    | IdentificaÃ§Ã£o Ãºnica do arremesso. |

##### ObservaÃ§Ãµes
- O dataset contÃ©m 24.271 registros.
- A coluna `shot_made_flag` possui valores nulos, indicando arremessos cuja conversÃ£o nÃ£o foi informada.

---

### 2. ğŸ“‘ **data_filtered** (CatÃ¡logo: `entrada`)
- **Tipo**: Dados prÃ©-processados.
- **ComposiÃ§Ã£o**:
  - Resultado da limpeza e filtragem realizada pelo pipeline `PreparacaoDados` no node `prepare_data`.
  - As transformaÃ§Ãµes incluem:
    - RemoÃ§Ã£o de outliers ou nulos.
    - ConversÃ£o de tipos de dados.
  - Cadastrado no `catalog.yml` como um dataset intermediÃ¡rio.

##### Somente as colunas foram selecionadas para o `data_filtered`:

| Coluna               | Tipo     | DescriÃ§Ã£o |
|----------------------|----------|-----------|
| lat                 | float64  | Latitude da posiÃ§Ã£o do arremesso na quadra. |
| lon                 | float64  | Longitude da posiÃ§Ã£o do arremesso na quadra. |
| minutes_remaining   | int64    | Minutos restantes no quarto em que o arremesso foi realizado. |
| period              | int64    | NÃºmero do perÃ­odo da partida (ex: 1, 2, 3, 4, OT). |
| playoffs           | int64    | Indica se a partida foi nos playoffs (1) ou na temporada regular (0). |
| shot_distance      | int64    | DistÃ¢ncia do arremesso em pÃ©s. |


### 3. ğŸ“‘ **base_train / base_test** (CatÃ¡logo: `entrada`)
- **Tipo**: Dados de treino e teste.
- **ComposiÃ§Ã£o**:
  - Conjunto de dados particionado a partir de `data_filtered`.
  - Usado para alimentar o node de `Treinamento`.
  - Armazenado separadamente em arquivos como `base_train.parquet` e `base_test.parquet`.


### 4. ğŸ§® **Modelo Treinado** (registrado no MLflow)
- **Tipo**: Modelo de machine learning.
- **ComposiÃ§Ã£o**:
  - Objeto serializado dos modelos `Logistic Regression` e `Decision Tree Classifier`.
  - MetainformaÃ§Ãµes como:
    - HiperparÃ¢metros usados.
    - MÃ©tricas de performance (ex: acurÃ¡cia, F1-score).
    - Artefatos complementares como scaler, encoder, etc.
  - Registrado no **MLflow Model Registry** com versionamento.


### 5. ğŸ“ˆ **Registro no MLflow**
- **Tipo**: Metadata tracking.
- **ComposiÃ§Ã£o**:
  - Logs de execuÃ§Ã£o do experimento.
  - ParÃ¢metros de input (hiperparÃ¢metros).
  - MÃ©tricas de avaliaÃ§Ã£o.
  - Arquivos de saÃ­da como o modelo `.pkl`, grÃ¡ficos, etc.
  - Interface visual e API de consulta.


### 6. ğŸ“Ÿ **AplicaÃ§Ã£o Streamlit**
- **Tipo**: Interface web.
- **ComposiÃ§Ã£o**:
  - CÃ³digo Python com lÃ³gica de front-end interativo.
  - Elementos como:
    - Campos de input do usuÃ¡rio.
    - LÃ³gica de requisiÃ§Ã£o ao modelo via MLflow (ou diretamente).
    - ExibiÃ§Ã£o de outputs com probabilidades.
  - Conectada ao modelo treinado para inferÃªncia em tempo real.


### 7. âš™ï¸ **Nodes (FunÃ§Ãµes do pipeline)**
Embora nÃ£o sejam arquivos em si, os *nodes* sÃ£o artefatos de cÃ³digo fundamentais:
- **PreparacaoDados**:
  - FunÃ§Ã£o responsÃ¡vel por ingestÃ£o e limpeza dos dados brutos.
- **Treinamento**:
  - FunÃ§Ãµes que recebem os dados tratados e executa o treinamento dos modelos.


## ğŸ¯QuestÃ£o 5

---
No prompt de comando, executado o seguinte comando:
```bash
kedro run --pipeline=PreparacaoDados
```

â„¹ï¸ Sobre o dataset `data_filtered` 
> DimensÃ£o do dataset apÃ³s tratamento: **_20285 linhas e 7 colunas_**.

### **ImplementaÃ§Ã£o e ExecuÃ§Ã£o do Pipeline "PreparacaoDados"**
Todo o pipeline foi integrado com MLFlow para registro e acompanhamento dos experimentos.
<img src="docs\images\pipeline-run-preparacao-log.png">

Ao executar o pipeline, as seguintes mÃ©tricas foram geradas no MLflow:
<img src="docs\images\mlflow-preparacao.png">

E tambÃ©m os arquivos gerados:
<img src="docs\images\pipeline-preparacao-artefatos.png">

### âœ… **EstratÃ©gias para Minimizar o ViÃ©s de Dados**
1. **DivisÃ£o Estratificada**  
   - Para problemas de classificaÃ§Ã£o, a tÃ©cnica **stratified split** garante que a distribuiÃ§Ã£o da variÃ¡vel alvo seja semelhante nos conjuntos de treino e teste.  
   - Isso evita que o modelo aprenda padrÃµes enviesados por distribuiÃ§Ãµes desbalanceadas.

   **Exemplo no Scikit-Learn:**  
   ```python
   from sklearn.model_selection import train_test_split

   train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["shot_made_flag"], random_state=42)
   ```

2. **Aumento de Dados (Data Augmentation)**  
   - Quando hÃ¡ poucas amostras de certas classes, pode-se gerar novos dados sintÃ©ticos para equilibrar as classes.
   - TÃ©cnicas como **SMOTE (Synthetic Minority Over-sampling Technique)** podem ser usadas para balancear classes desbalanceadas.

3. **Cross-Validation (ValidaÃ§Ã£o Cruzada)**  
   - Em vez de usar uma Ãºnica divisÃ£o treino/teste, a validaÃ§Ã£o cruzada (ex.: k-fold cross-validation) permite testar o modelo em diferentes divisÃµes dos dados, reduzindo o risco de viÃ©s na amostragem.

   **Exemplo de K-Fold Cross-Validation no Scikit-Learn:**  
   ```python
   from sklearn.model_selection import KFold, cross_val_score

   kf = KFold(n_splits=5, shuffle=True, random_state=42)
   scores = cross_val_score(model, X, y, cv=kf, scoring="accuracy")
   print(f"MÃ©dia das acurÃ¡cias: {scores.mean()}")
   ```

4. **RemoÃ§Ã£o de Dados Redundantes e Limpeza**  
   - Identificar e remover duplicatas ou dados inconsistentes pode evitar que o modelo aprenda padrÃµes irrelevantes.

5. **Testar com Dados de ProduÃ§Ã£o**  
   - Se possÃ­vel, testar o modelo com dados reais que ele encontrarÃ¡ na produÃ§Ã£o para garantir que os resultados sejam confiÃ¡veis.

## ğŸ¯QuestÃ£o 6

---
No prompt de comando, executado o seguinte comando:
```bash
kedro run --pipeline=Treinamento
```

O pipeline foi deviamente registrado no MLflow:
<img src="docs\images\mlflow-treinamento.png">

Durante a execuÃ§Ã£o do pipeline, o seguinte output no console foi gerado:

#### âœ… **Ãrvore de DecisÃ£o:**
<img src="docs\images\training-metrics-dt.png">

---

#### âœ…**RegeressÃ£o LogÃ­stica:**
<img src="docs\images\training-metrics-lr.png">

---
Com base nas mÃ©tricas de validaÃ§Ã£o cruzada (10 folds) apresentadas nas imagens dos dois modelos â€” **RegressÃ£o LogÃ­stica** e **Ãrvore de DecisÃ£o** â€” o modelo mais adequado para finalizaÃ§Ã£o Ã©:

### âœ… **Modelo Escolhido: RegressÃ£o LogÃ­stica**

##### **Comparativo das principais mÃ©tricas (mÃ©dia dos folds)**

| MÃ©trica     | RegressÃ£o LogÃ­stica | Ãrvore de DecisÃ£o |
|-------------|----------------------|--------------------|
| **Accuracy** | **0.5781**           | 0.5264             |
| **AUC**      | **0.6085**           | 0.5160             |
| **Recall**   | 0.4921               | **0.5530**         |
| **Precision**| **0.5669**           | 0.5034             |
| **F1 Score** | **0.5267**           | 0.5142             |
| **Kappa**    | **0.1496**           | 0.0657             |
| **MCC**      | **0.1509**           | 0.0662             |


### ğŸ§  **Justificativa da escolha**

1. **AcurÃ¡cia Geral Superior**: A regressÃ£o logÃ­stica alcanÃ§ou uma mÃ©dia de acurÃ¡cia 5 pontos percentuais acima da Ã¡rvore de decisÃ£o (57.8% vs. 52.6%).

2. **Melhor SeparaÃ§Ã£o entre Classes (AUC)**: O AUC da regressÃ£o logÃ­stica (0.6085) indica maior capacidade de distinguir as classes corretamente. O modelo de Ã¡rvore tem AUC prÃ³ximo de 0.5, o que sugere desempenho similar ao aleatÃ³rio.

3. **MÃ©tricas de Balanceamento (Kappa e MCC)**: Ambas sÃ£o substancialmente mais altas na regressÃ£o logÃ­stica, o que reforÃ§a que o modelo estÃ¡ aprendendo padrÃµes Ãºteis, e nÃ£o apenas se ajustando ao desbalanceamento ou aleatoriedade.

4. **Recall ligeiramente inferior, mas compensado**: Embora a Ã¡rvore de decisÃ£o tenha maior *recall* (sensibilidade), ela perde em todas as outras mÃ©tricas, o que torna o modelo menos robusto como um todo.

---

## ğŸ¯QuestÃ£o 7

ApÃ³s desenvolver e executar o pipeline, o seguinte erro foi gerado no console:
<img src="docs\images\pipeline-aplicacao-erro.png">

Significa que os tipos de dados do DataFrame de produÃ§Ã£o nÃ£o Ã© aderente ao de desenvolvimento. Isso ocorreu pois os tipos de dados passados para o modelo via predict nÃ£o bateram com a assinatura registrada do modelo.

Para contornar o problema de falha na execuÃ§Ã£o, foi necessÃ¡rio editar o cÃ³digo para corrigir os tipos de dados.
<img src="docs\images\mlflow-aplicacao-plot.png">

### ğŸ“Œ **O que mudou entre a base de treino (`kobe_dev`) e a base de produÃ§Ã£o (`kobe_prod`)?**

Pelos indÃ­cios observados:

| Aspecto | Base de Treinamento (`dev`) | Base de ProduÃ§Ã£o (`prod`) |
|--------|------------------------------|----------------------------|
| **Colunas** | 7 (features + target) | 24 (colunas completas do dataset original) |
| **Filtragem** | PrÃ©-processada, com somente variÃ¡veis relevantes e sem nulos | Crua, sem filtragem, com valores faltantes no `shot_made_flag` |
| **Tipos de dados** | Todos como `float64`, compatÃ­veis com MLflow | Muitos como `int64`, `object`, e com `NaN`s |

#### ğŸ§  ConclusÃ£o:
A base de produÃ§Ã£o **nÃ£o foi tratada com o mesmo prÃ©-processamento** que a base de treino, o que pode causar inconsistÃªncia nas previsÃµes e quebra de performance.

#### ğŸ§  Justificativa

O modelo **nÃ£o Ã© diretamente aderente** Ã  base de produÃ§Ã£o por trÃªs motivos principais:

1. A produÃ§Ã£o nÃ£o passou por prÃ©-processamento (tem campos extras, tipos errados e valores nulos).
2. As variÃ¡veis relevantes estÃ£o misturadas com outras nÃ£o usadas pelo modelo.
3. A presenÃ§a de `NaN` no target impedia a avaliaÃ§Ã£o direta do desempenho.


### âœ… **Monitoramento da saÃºde do modelo**

A saÃºde de um modelo pode ser monitorada em duas frentes principais:

- **Performance**: como o modelo estÃ¡ se saindo?
- **Dados**: os dados que entram no modelo continuam parecidos com os que ele foi treinado?


### **1. Quando a variÃ¡vel resposta (target) estÃ¡ disponÃ­vel em produÃ§Ã£o**

Este Ã© o **melhor cenÃ¡rio possÃ­vel**, pois permite **medir o desempenho real** do modelo com dados atualizados.

#### ğŸ¯ O que pode ser monitorado:
- **MÃ©tricas de performance** como:
  - `log_loss`, `f1_score`, `accuracy`, `precision`, `recall`
- **Atrasos entre previsÃ£o e rÃ³tulo real** (tempo de feedback)
- **Drift de performance**: comparar as mÃ©tricas com benchmarks anteriores

#### ğŸ› ï¸ Ferramentas e abordagens:
- Executar o **pipeline de aplicaÃ§Ã£o** com o target incluÃ­do
- Registrar as mÃ©tricas no MLflow ou ferramentas como Evidently, Prometheus, etc.
- Programar **dashboards** em Streamlit, Superset, Grafana ou Power BI

---

### **2. Quando a variÃ¡vel resposta NÃƒO estÃ¡ disponÃ­vel em produÃ§Ã£o**

Neste caso, vocÃª **nÃ£o pode medir diretamente a performance**. Mas ainda Ã© possÃ­vel monitorar **a integridade e a aderÃªncia dos dados**.

#### ğŸ” O que monitorar:

##### a) **Data Drift (mudanÃ§a nos dados de entrada)**
- MudanÃ§a na distribuiÃ§Ã£o das variÃ¡veis
- Novos valores em variÃ¡veis categÃ³ricas
- MudanÃ§a de mÃ©dia, mediana, desvio padrÃ£o

##### b) **Feature Importance Drift**
- Ver se a importÃ¢ncia das variÃ¡veis mudou muito com o tempo

##### c) **Score Stability**
- A distribuiÃ§Ã£o das probabilidades do modelo ao longo do tempo
- Ex: mÃ©dia da probabilidade de classe 1

#### ğŸ› ï¸ Ferramentas e abordagens:
- Usar **Evidently** (Python) para:
  - Data Drift Report
  - Targetless Monitoring
- **Alerts automÃ¡ticos** se alguma feature sair do intervalo de confianÃ§a
- Logar a entrada do modelo com timestamp para auditoria futura


#### ğŸ“Š Exemplo prÃ¡tico de mÃ©tricas monitorÃ¡veis sem `y`:

| MÃ©trica                 | Com `y` | Sem `y` |
|------------------------|---------|---------|
| Accuracy / F1 Score    | âœ…      | âŒ      |
| Log Loss               | âœ…      | âŒ      |
| Probabilidade mÃ©dia    | âœ…/âŒ   | âœ…      |
| MudanÃ§a de distribuiÃ§Ã£o| âœ…/âŒ   | âœ…      |
| Tempo de prediÃ§Ã£o      | âœ…      | âœ…      |
| Taxa de erros tÃ©cnicos | âœ…      | âœ…      |


### ğŸ§© ConclusÃ£o

| SituaÃ§Ã£o                     | O que monitorar                                |
|-----------------------------|------------------------------------------------|
| **Com target disponÃ­vel**   | MÃ©tricas de performance + integridade de dados |
| **Sem target disponÃ­vel**   | Drift de dados + distribuiÃ§Ã£o de scores        |

Ambos os cenÃ¡rios exigem aÃ§Ãµes automÃ¡ticas como **logs, alertas e auditorias**, e ferramentas como **MLflow, Evidently, Prometheus e Streamlit** ajudam nesse processo.

---
Quando colocamos um modelo em produÃ§Ã£o, Ã© fundamental definir **estratÃ©gias de retreinamento** para garantir que ele continue relevante, preciso e confiÃ¡vel com o passar do tempo.

Essas estratÃ©gias podem ser divididas em **duas abordagens principais**:

### ğŸ” **1. EstratÃ©gia Reativa de Retreinamento**

> O modelo sÃ³ Ã© retreinado **quando hÃ¡ sinais claros de degradaÃ§Ã£o**.

#### ğŸ“Œ CaracterÃ­sticas:
- Baseada em **monitoramento constante** da performance
- Retreinamento **acontece apÃ³s** o modelo apresentar queda nas mÃ©tricas
- Depende da **disponibilidade da variÃ¡vel resposta (target)**

#### ğŸ§  Exemplos de gatilhos:
- F1 Score ou Accuracy caiu abaixo de um limite (ex: 5% abaixo do benchmark)
- Log Loss subiu acima de um limiar
- Aumento de reclamaÃ§Ãµes ou erros operacionais
- Atraso no tempo de resposta devido a complexidade crescente

#### âœ… Vantagens:
- Evita retrabalho desnecessÃ¡rio
- Ã‰ eficiente quando hÃ¡ **feedback contÃ­nuo** do target

#### âŒ Desvantagens:
- Ã‰ **reativa**: o modelo jÃ¡ degradou quando o retreinamento comeÃ§a
- Pode gerar **impactos negativos** antes da correÃ§Ã£o (ex: perda de vendas, decisÃµes ruins)


### ğŸ“… **2. EstratÃ©gia Preditiva (ou Proativa) de Retreinamento**

> O modelo Ã© retreinado **em ciclos planejados ou com base em sinais antecipados**, mesmo sem queda visÃ­vel de performance.

#### ğŸ“Œ CaracterÃ­sticas:
- Foco em **prevenir** problemas futuros
- Usa anÃ¡lise de **data drift** ou **mudanÃ§a de contexto**
- Pode ser baseada em **agendamento (ex: mensal, trimestral)**

#### ğŸ§  Exemplos de gatilhos:
- MudanÃ§a estatÃ­stica significativa nas variÃ¡veis de entrada (data drift)
- Aumento na frequÃªncia de valores ausentes, anomalias ou categorias novas
- Modelo em produÃ§Ã£o atingiu um nÃºmero X de novas amostras
- **Janela de tempo definida** (retraining a cada 60 dias, por exemplo)

#### âœ… Vantagens:
- MantÃ©m o modelo sempre fresco e adaptado
- Reduz risco de deterioraÃ§Ã£o sÃºbita

#### âŒ Desvantagens:
- Pode ser **custoso** (mais consumo computacional e tempo de validaÃ§Ã£o)
- Risco de **overfitting ao novo contexto** se nÃ£o for bem gerenciado

---

#### ğŸ§© Comparativo das EstratÃ©gias

| Aspecto                    | EstratÃ©gia Reativa        | EstratÃ©gia Preditiva         |
|---------------------------|---------------------------|------------------------------|
| Base                      | MÃ©tricas de performance   | Tempo, volume ou sinais de drift |
| Exige `target`?           | Sim                       | NÃ£o necessariamente          |
| AÃ§Ã£o                      | ApÃ³s problema aparecer    | Antes do problema acontecer  |
| Custo computacional       | Mais baixo                | Mais alto                    |
| Risco de impacto negativo | Maior                     | Menor                        |

---

### ğŸ“Ÿ Streamlit App

Foi implementada uma aplicaÃ§Ã£o Streamlit para visualizaÃ§Ã£o dos dados e monitoramento do modelo.
O App possui 2 abas:

- **Aba 1**: PrevisÃ£o --> (Consome a API do MLflow)
- **Aba 2**: Monitoramento da OperaÃ§Ã£o

> Para executar a app, basta rodar o arquivo `streamlit/app.py` com o comando: 
```bash
streamlit run app.py
```

<img src="docs\images\streamlit-aba-prev.png">
<img src="docs\images\streamlit-aba-mon-01.png">
<img src="docs\images\streamlit-aba-mon-02.png">
<img src="docs\images\streamlit-aba-mon-03.png">